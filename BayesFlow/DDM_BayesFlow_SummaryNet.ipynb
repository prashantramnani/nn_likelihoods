{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "# print(tqdm.__version__)\n",
    "\n",
    "from models import DeepConditionalModel, InvariantNetwork\n",
    "from losses import maximum_likelihood_loss\n",
    "from inn_utils import train_online_ml\n",
    "# from viz import plot_losses, plot_metrics_params\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "import psutil\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import scipy as scp\n",
    "from scipy.stats import gamma\n",
    "import time\n",
    "\n",
    "import cddm_data_simulation as cds\n",
    "import kde_training_utilities as kde_util\n",
    "import kde_class as kde\n",
    "import boundary_functions as bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator function for 2-choice data using ddm_flexbound\n",
    "def data_generator_ddm_flexbound(batch_size):\n",
    "    v = np.random.uniform(-3, 3, batch_size)\n",
    "    a = np.random.uniform(0.3, 2.5, batch_size)\n",
    "    w = np.random.uniform(0.1, 0.9, batch_size)\n",
    "    \n",
    "    # Number of paths to be sampled for each batch    \n",
    "    n_samples = 10000 \n",
    "    \n",
    "    # Bool to determine how to put 'rt' and 'choice_made' together\n",
    "    multiply = True \n",
    "    \n",
    "    boundary_function = bf.constant\n",
    "        \n",
    "    X_train = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        out = cds.ddm_flexbound(v[i], \n",
    "                                a[i], \n",
    "                                w[i],\n",
    "                                ndt = 0.5,\n",
    "                                delta_t = 0.001, \n",
    "                                s = np.sqrt(2),\n",
    "                                max_t = 20,\n",
    "                                n_samples = n_samples,\n",
    "                                boundary_fun = boundary_function,\n",
    "                                boundary_multiplicative = True, \n",
    "                                boundary_params = {})\n",
    "                                #boundary_params = {\"theta\": 0.01})\n",
    "        if multiply:\n",
    "            # Multiply 'rt' and 'choice_made'\n",
    "            data = (out[0]*out[1]).reshape(n_samples, )\n",
    "        else:        \n",
    "            # concatenate 'rt' and 'choice_made'\n",
    "            data = np.concatenate((out[0].T, out[1].T), axis=1).reshape(2*n_samples,) \n",
    "            \n",
    "        X_train.append(data)    \n",
    "        \n",
    "    X_train = np.array(X_train)         \n",
    "    # Concatenating a, v and w\n",
    "    param = np.concatenate((a.reshape(-1, 1), v.reshape(-1, 1), w.reshape(-1, 1)), axis=1)\n",
    "    \n",
    "    return tf.convert_to_tensor(X_train, dtype=tf.float32), tf.convert_to_tensor(param, dtype=tf.float32)\n",
    "    \n",
    "\n",
    "# start = time.time()\n",
    "# a, b = data_generator_ddm_flexbound(100)    \n",
    "# end = time.time()\n",
    "# print(\"Data generation took: {} time\".format(end - start))\n",
    "# print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_opt(n_inv_blocks, summary_dim, global_step):\n",
    "    \"\"\"Loads a GMM model given the number of invertible blocks.\"\"\"\n",
    "    \n",
    "    # Create model\n",
    "    summary_net = InvariantNetwork(summary_dim, n_equiv=3)\n",
    "    model = DeepConditionalModel(inv_meta, n_inv_blocks, theta_dim, summary_net=None, permute=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    # Checkpoint model\n",
    "    checkpoint = tf.train.Checkpoint(step=global_step, optimizer=optimizer, net=model)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, './checkpoints/ddm_flexbound_summarynet_{}'.format(n_inv_blocks), max_to_keep=2)\n",
    "    checkpoint.restore(manager.latest_checkpoint)\n",
    "    \n",
    "    return model, optimizer, manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_inv_blocks, summary_dim):\n",
    "    \"\"\"\n",
    "    Runs the Gausian Distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    model, optimizer, manager = load_model_and_opt(n_inv_blocks, summary_dim, global_step)\n",
    "    \n",
    "    for ep in range(1, epochs+1):\n",
    "        with tqdm(total=iterations_per_epoch, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "\n",
    "            # Run training loop\n",
    "            train_online_ml(model, optimizer, data_generator_ddm_flexbound, iterations_per_epoch, \n",
    "                            batch_size, p_bar=p_bar, clip_value=clip_value, global_step=global_step, \n",
    "                            transform=None, n_smooth=100)\n",
    "            \n",
    "            manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the various configurations\n",
    "inv_meta = {\n",
    "    'n_units': [128, 128, 128],\n",
    "    'activation': 'elu',\n",
    "    'w_decay': 0.0,\n",
    "    'initializer': 'glorot_uniform'\n",
    "}\n",
    "\n",
    "n_inv = 10\n",
    "theta_dim = 3\n",
    "# params_names = [r'$\\mu_{}$'.format(i+1) for i in range(theta_dim)]\n",
    "global_step = tf.Variable(0, dtype=tf.int32)\n",
    "batch_size = 50\n",
    "summary_dim = 128\n",
    "epochs = 20\n",
    "iterations_per_epoch = 100\n",
    "n_samples_posterior = 2000\n",
    "starter_learning_rate = 0.001\n",
    "decay_steps = 1000\n",
    "decay_rate = .99\n",
    "clip_value = 5.\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, \n",
    "                                           decay_steps, decay_rate, staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d379cba1ff854ccb9f3ae3afcbbd9032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training epoch 1', style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/array_grad.py:562: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ff6c98d548bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_inv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-7568960ebf8a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(n_inv_blocks, summary_dim)\u001b[0m\n\u001b[1;32m     12\u001b[0m             train_online_ml(model, optimizer, data_generator_ddm_flexbound, iterations_per_epoch, \n\u001b[1;32m     13\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_bar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                             transform=None, n_smooth=100)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Intern/HDDM_nn/nn_likelihoods/BayesFlow/inn_utils.py\u001b[0m in \u001b[0;36mtrain_online_ml\u001b[0;34m(model, optimizer, data_generator, iterations, batch_size, p_bar, clip_value, global_step, transform, n_smooth)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# Generate data and parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;31m# Apply some transformation, if specified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c83fdd6c63cb>\u001b[0m in \u001b[0;36mdata_generator_ddm_flexbound\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m                                 \u001b[0mboundary_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboundary_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                 \u001b[0mboundary_multiplicative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                                 boundary_params = {})\n\u001b[0m\u001b[1;32m     29\u001b[0m                                 \u001b[0;31m#boundary_params = {\"theta\": 0.01})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(n_inv, summary_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hddm",
   "language": "python",
   "name": "hddm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
